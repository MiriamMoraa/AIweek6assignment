— Prototype & Deliverables

Repository structure (recommended)
edge-ai-recycling/
├─ notebook.ipynb        # Jupyter notebook (export from this markdown)
├─ README.md             # Project README (detailed)
 ├─ requirements.txt     # Python dependencies
├─ scripts/
│   ├─ convert_and_test.py
│   └─ pi_infer.py
├─ models/
│   └─ (put model_float32.tflite, model_int8.tflite here)
├─ data/
│   └─ (dataset pointers, small sample images if permitted)
└─ report.md             # Short report with results, figures (optional)
README.md
# Edge AI Recycling Classifier — Prototype




Contents
- `notebook.ipynb` — Jupyter notebook with training, evaluation, and TFLite conversion steps.
- `scripts/convert_and_test.py` — Script to convert a Keras model to TFLite and run a quick local test.
- `scripts/pi_infer.py` — Lightweight inference script compatible with Raspberry Pi `tflite-runtime`.
- `requirements.txt` — Python dependencies, pinned versions.
- `models/` — Store converted `.tflite` models here.





# requirements.txt


Training environment (Colab/GPU)

tensorflow>=2.12.0,<2.14.0 numpy pandas matplotlib scikit-learn Pillow

For Raspberry Pi inference using tflite-runtime (install on Pi, not required on Colab)
tflite-runtime


    float_path = os.path.join(out_dir, 'model_float32.tflite')
    with open(float_path, 'wb') as f:
        f.write(tflite_model)
    print('Saved float32 TFLite to', float_path)


    # Integer quantization (if representative dataset provided)
    if rep_dir and os.path.isdir(rep_dir):
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.representative_dataset = lambda: representative_dataset_gen(rep_dir, img_size)
        # Target full integer
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.uint8
        converter.inference_output_type = tf.uint8
        tflite_quant = converter.convert()
        int8_path = os.path.join(out_dir, 'model_int8.tflite')
        with open(int8_path, 'wb') as f:
            f.write(tflite_quant)
        print('Saved int8 quantized TFLite to', int8_path)
    else:
        print('Representative dataset not found or not provided; skipping int8 conversion.')




def quick_infer(tflite_path, img_path, img_size=(160,160)):
    # Use full TF interpreter on desktop
    interpreter = tf.lite.Interpreter(model_path=tflite_path)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()


    img = Image.open(img_path).convert('RGB').resize(img_size)
    input_shape = input_details[0]['shape']
    # Handle uint8 inputs for quantized model
    if input_details[0]['dtype'] == np.uint8:
        input_data = np.expand_dims(np.array(img), axis=0).astype(np.uint8)
    else:
        input_data = np.expand_dims(np.array(img).astype(np.float32), axis=0)


    interpreter.set_tensor(input_details[0]['index'], input_data)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details[0]['index'])
    print('Inference output:', output)




if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Convert and test Keras model to TFLite')
    parser.add_argument('--keras_model', required=True, help='Path to Keras model (.h5 or SavedModel dir)')
    parser.add_argument('--out_dir', default='./models', help='Output directory for tflite models')
    parser.add_argument('--rep_dir', default='./data/rep_images', help='Representative images dir')
    parser.add_argument('--test_image', default=None, help='Optional test image for quick inference')
    args = parser.parse_args()


    model = load_keras_model(args.keras_model)
    convert_model(model, args.out_dir, args.rep_dir)
    if args.test_image:
        # Prefer quantized model if present
        candidate = os.path.join(args.out_dir, 'model_int8.tflite')
        if not os.path.exists(candidate):
            candidate = os.path.join(args.out_dir, 'model_float32.tflite')
        quick_infer(candidate, args.test_image)
scripts/pi_infer.py
"""pi_infer.py


Lightweight TFLite inference script for Raspberry Pi using tflite-runtime.


Example:
  python3 pi_infer.py --model models/model_int8.tflite --image test.jpg


This script is intentionally small and avoids heavy imports so it runs on Pi.
"""


import argparse
import numpy as np
from PIL import Image


try:
    import tflite_runtime.interpreter as tflite
except Exception:
    # If tflite_runtime is not available, fall back to full TF (on desktops)
    import tensorflow as tf
    tflite = tf.lite




def load_image(img_path, size=(160,160), input_dtype=np.uint8):
    img = Image.open(img_path).convert('RGB').resize(size)
    arr = np.array(img)
    if input_dtype == np.uint8:
        return arr.astype(np.uint8)
    else:
        return arr.astype(np.float32)




def run_inference(model_path, image_path):
    interpreter = tflite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()


    in_type = input_details[0]['dtype']
    img = load_image(image_path, size=tuple(input_details[0]['shape'][1:3]), input_dtype=in_type)
    input_data = np.expand_dims(img, axis=0).astype(in_type)


    interpreter.set_tensor(input_details[0]['index'], input_data)
    import time
    t0 = time.time()
    interpreter.invoke()
    t1 = time.time()
    output = interpreter.get_tensor(output_details[0]['index'])


    print('Pred:', output)
    print('Latency: {:.3f} ms'.format((t1 - t0) * 1000))




if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', required=True, help='Path to .tflite model')
    parser.add_argument('--image', required=True, help='Path to image for inference')
    args = parser.parse_args()
    run_inference(args.model, args.image)
notebook.ipynb (Notebook content as Markdown + code cells)


Notebook: Edge AI — Recycling Classifier (MobileNetV2 Transfer Learning)
1. Setup & Imports
# Cell 1
import os
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report


print('TensorFlow version', tf.__version__)
2. Config / Hyperparameters
# Cell 2
IMG_SIZE = (160,160)
BATCH_SIZE = 32
EPOCHS = 15
NUM_CLASSES = 5  # adjust to your dataset
DATA_DIR = '/content/data/'  # in Colab, upload dataset to this path or mount Drive
